{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c1ba60",
   "metadata": {},
   "source": [
    "Load the InceptionV3 model with pretrained ImageNet weights, unfreeze some top layers, and fine-tune it on a custom binary dataset (e.g., cats vs dogs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe6e626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a006727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "IMG_SIZE = 299\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fb23bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset directories\n",
    "train_dir='./cats_and_dogs_filtered/train'\n",
    "val_dir='./cats_and_dogs_filtered/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eec8d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "train_ds=tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Load validation dataset\n",
    "val_ds=tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size=(IMG_SIZE,IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5135b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation=Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8664f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing layer for InterceptionV3\n",
    "preprocess_layer=layers.Rescaling(1./127.5,offset=-1)\n",
    "# Prepare datasets (efficient pipeline)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.map(lambda x, y: (preprocess_layer(x), y)).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda x, y: (preprocess_layer(x), y)).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d0b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87910968/87910968 [==============================] - 11s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load InceptionV3 base mode\n",
    "base_model=InceptionV3(weights='imagenet',include_top=False,input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "base_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30dfca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "    data_augmentation,\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32b6b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f0710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "63/63 [==============================] - 384s 6s/step - loss: 0.1414 - accuracy: 0.9475 - val_loss: 0.0237 - val_accuracy: 0.9940\n",
      "Epoch 2/3\n",
      "30/63 [=============>................] - ETA: 1:50 - loss: 0.0562 - accuracy: 0.9831"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918100d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f489ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile with lower learning rate for fine-tuning\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in val_ds.take(1):\n",
    "    pred = model.predict(images[:1])\n",
    "    plt.imshow(tf.cast(images[0]*127.5+127.5, tf.uint8))\n",
    "    plt.title(f\"Predicted: {'Dog' if pred[0][0] > 0.5 else 'Cat'}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
